{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is my first (public) notebook of playing with pyspark\n",
    "\n",
    "The goal of this notebook is to demonstrare a simple use case of:\n",
    "1. using pyspark (python wrapper for spark) and access existing spark cluster\n",
    "2. loading data into spark\n",
    "3. accessing the data using the rdd-api, dataframe-api, and dataset-api\n",
    "4. simple data exploration\n",
    "\n",
    "I'll be using a sample of the gutenberg project - https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html\n",
    "\n",
    "#### The settings I'm using here:\n",
    "- spark: version 2.0.0\n",
    "- python: 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import all the relevant packages\n",
    "import pyspark #python wrapper for spark\n",
    "#import urllib3 #ability to access data by url\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link to existing spark cluster\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(appName=\"spark-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of Told in a French Garden, by Mildred Aldrich']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we demonstrare a usage of the RDD API to read a text file\n",
    "rdd = sc.textFile(\"s3n://ak-public-sandbox/datasets/gutenberg_dataset/1/8/*/*/*\")\n",
    "rdd.count()\n",
    "rdd.take(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see some fitering capabilities\n",
    "rdd.filter(lambda line: \"French\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The good-old word count example\n",
    "term_freq = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a,b: a + b)\n",
    "print(\"Total number of unique words %s\" % term_freq.count())\n",
    "stop_words = term_freq.takeOrdered(5,lambda a:-a[1])\n",
    "rare_words = term_freq.takeOrdered(5,lambda a:a[1])\n",
    "print(\"5 most frequent words: (word, freq): {0}\".format(stop_words))\n",
    "print(\"5 least frequent words: (word, freq) {0}\".format(rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We count the number of occurences of each frequect (by value)\n",
    "# For example, the term 'the' is the only which occured 37335, so we denote (37335,1).\n",
    "count_per_freq = term_freq.map(lambda tf: (tf[1],1)).reduceByKey(lambda a,b: a+b)\n",
    "# We use very simple bucketing, this is needed due to the collect call in the next cell - to avoid crashing the master node due to OOM\n",
    "bucket_size = 50\n",
    "binned_counter = count_per_freq.map(lambda freq: (freq[0] / bucket_size,freq[1])).reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to plot the (binned) frequency data, using log\n",
    "num_bins = 100\n",
    "n, bins, patches = plt.hist(binned_counter.values().collect(), num_bins, facecolor='b', normed=1,alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ability to save the rdd directly as text file in s3\n",
    "#rdd.saveAsTextFile(\"s3n://full/path/here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic usage of boto3\n",
    "#import boto3\n",
    "#session = boto3.Session() # leave out the profile_name argument if you haven't defined profiles\n",
    "#s3 = boto3.resource('s3')\n",
    "#bucket = s3.Bucket(bucket_name)\n",
    "#objs = bucket.objects.filter(Prefix=some_prefix)\n",
    "#for obj in objs:  \n",
    "#  print(obj.key)\n",
    "#obj = s3.Object(bucket_name, file_full_path_in_bucket)\n",
    "#content = obj.get()[\"Body\"].read().decode(\"utf-8\")\n",
    "#print(content[:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 195,
   "position": {
    "height": "40px",
    "left": "729.984px",
    "right": "20px",
    "top": "74px",
    "width": "526px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
